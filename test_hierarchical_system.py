"""
Test the coordinator agent with PID motor control.

Tests waypoint navigation with the trained coordinator agent.
The coordinator issues velocity commands which are executed by a PID controller.
"""

import numpy as np
from src.envs.action_coordinator_env import ActionCoordinatorEnv
from stable_baselines3 import PPO
try:
    from sb3_contrib import RecurrentPPO
    HAS_RECURRENT = True
except ImportError:
    HAS_RECURRENT = False
    print("Warning: sb3-contrib not installed, cannot load RecurrentPPO models")
import time
import pybullet as p

def test_hierarchical_system():
    """Test the coordinator agent with PID motor control."""
    
    # Load coordinator model - no motor agent needed (uses PID controller)
    try:
        if HAS_RECURRENT:
            from sb3_contrib import RecurrentPPO as RPPO
            
            # Try to load the most recent coordinator model
            try:
                coord_model = RPPO.load("models/experiment_v13_checkpoints/coordinator_350000_steps")
                print("âœ“ Loaded experiment_v13 coordinator model (RecurrentPPO with GRU + Acceleration)")
                print("  Control mode: Acceleration commands â†’ PID controller\n")
            except FileNotFoundError:
                try:
                    coord_model = RPPO.load("models/experiment_v12_checkpoints/coordinator_350000_steps")
                    print("âš  Loaded experiment_v12 (INCOMPATIBLE - 522D obs vs 526D env)")
                    print("  This will likely fail. Train v13 with: python -m src.train --timesteps 400000 --name experiment_v13\n")
                except FileNotFoundError:
                    try:
                        coord_model = RPPO.load("models/coordinator_v1_final")
                        print("âš  Loaded coordinator_v1 (OLD - velocity commands)")
                        print("  Control mode: Velocity commands â†’ PID controller\n")
                    except FileNotFoundError:
                        print("âœ— No coordinator model found!")
                        print("  Train a model first: python -m src.train --timesteps 400000 --name experiment_v13")
                        return
        else:
            # No recurrent support
            try:
                coord_model = PPO.load("models/experiment_v1_final")
                print("âœ“ Loaded experiment_v1 coordinator model (PPO)")
                print("  Motor control: PID controller (no trained model)\n")
            except FileNotFoundError:
                print("âœ— No coordinator model found!")
                print("  Train a model first: python -m src.train --timesteps 200000 --name experiment_v1")
                return
    except Exception as e:
        print(f"âœ— Error loading model: {e}")
        return
    
    # Create coordinator environment (no motor_model_path needed - uses PID)
    env = ActionCoordinatorEnv(
        gui=True,
        enable_vision=True,  # Enable vision to match trained model
        enable_streaming=True,  # Stream stereo vision to VLC
        enable_obstacles=True,  # Show obstacles
        num_obstacles=10
    )
    
    print("\nðŸ“¹ Stereo vision streaming enabled!")
    print("   Open in VLC: http://localhost:5555/stream")
    print("   (Menu: Media â†’ Open Network Stream â†’ paste URL)\n")
    
    # Test scenarios - using same generation method as training
    test_scenarios = [
        {
            'name': 'Random Waypoints with Obstacles (Training-like)',
            'waypoints': None,  # Will be generated by reset()
            'duration': 1000,
            'num_tests': 3  # Run 3 random scenarios
        },
    ]
    
    print("="*80)
    print("COORDINATOR AGENT TEST: Waypoint Navigation with Acceleration Commands")
    print("="*80)
    print(f"Environment: 526D obs (14D base + 512D vision)")
    print(f"Action space: Acceleration commands [ax, ay, az, yaw_accel]")
    print(f"Max accel: Â±1.5 m/sÂ² linear, Â±Ï€/4 rad/sÂ² angular")
    print("="*80)
    
    overall_results = []
    
    for scenario in test_scenarios:
        num_tests = scenario.get('num_tests', 1)
        
        for test_num in range(num_tests):
            print(f"\n{'='*80}")
            print(f"Scenario: {scenario['name']}")
            if num_tests > 1:
                print(f"Test run: {test_num + 1}/{num_tests}")
            print(f"{'='*80}")
            
            # Reset environment (generates random waypoints and obstacles)
            obs, info = env.reset()
            
            # Display generated waypoints
            print(f"Waypoints: {len(env.waypoints)}")
            for i, wp in enumerate(env.waypoints):
                print(f"  WP{i}: [{wp[0]:+.1f}, {wp[1]:+.1f}, {wp[2]:+.1f}]")
            print(f"{'='*80}")
            
            # Initialize LSTM state for RecurrentPPO models
            # For RecurrentPPO, we need to track episode_start and lstm_states
            lstm_states = None
            episode_start = np.ones((1,), dtype=bool)
            
            waypoints_reached = 0
            total_distance_to_waypoints = 0
            position_history = []
            velocity_commands = []
            step_count = 0
            terminated = False  # Initialize to avoid unbound variable
            truncated = False
            
            for step in range(scenario['duration']):
                # Get current state
                state = env._getDroneStateVector(0)
                pos = state[:3]
                vel = state[10:13]
                position_history.append(pos.copy())
                
                # Update camera to follow drone
                camera_distance = 1.0
                camera_yaw = 45
                camera_pitch = -30
                p.resetDebugVisualizerCamera(
                    cameraDistance=camera_distance,
                    cameraYaw=camera_yaw,
                    cameraPitch=camera_pitch,
                    cameraTargetPosition=pos.tolist()
                )
                
                # Get velocity command from coordinator
                # RecurrentPPO needs episode_start and lstm_states
                is_recurrent = HAS_RECURRENT and hasattr(coord_model, 'policy') and hasattr(coord_model.policy, 'lstm_actor')
                if is_recurrent:
                    action, lstm_states = coord_model.predict(
                        obs, 
                        state=lstm_states,
                        episode_start=episode_start,
                        deterministic=True
                    )
                    episode_start = np.zeros((1,), dtype=bool)  # Only True on first step
                else:
                    action, _ = coord_model.predict(obs, deterministic=True)
                velocity_commands.append(action.copy())
                
                # Execute action (coordinator commands motor controller internally)
                obs, reward, terminated, truncated, info = env.step(action)
                step_count += 1
                
                # Track waypoints reached
                current_wp = env.waypoints[env.current_waypoint_idx]
                dist_to_wp = np.linalg.norm(pos - current_wp)
                total_distance_to_waypoints += dist_to_wp
                
                if env.waypoints_reached > waypoints_reached:
                    waypoints_reached = env.waypoints_reached
                    print(f"  âœ“ Waypoint {waypoints_reached} reached at step {step}")
                
                # Print progress
                if step % 50 == 0:
                    print(f"  Step {step:3d}: "
                          f"pos=[{pos[0]:+.2f}, {pos[1]:+.2f}, {pos[2]:+.2f}], "
                          f"vel_cmd=[{action[0]:+.2f}, {action[1]:+.2f}, {action[2]:+.2f}], "
                          f"dist_to_wp={dist_to_wp:.2f}m, "
                          f"reward={reward:+.2f}")
                
                time.sleep(0.02)  # Slow down for visualization
                
                if terminated:
                    print(f"\n  Episode terminated at step {step}")
                    print(f"  Reason: Crashed or out of bounds")
                    break
                
                if truncated:
                    print(f"\n  Episode truncated at step {step}")
                    break
            
            # Calculate statistics
            completion_rate = waypoints_reached / len(env.waypoints) * 100
            avg_dist_to_wp = total_distance_to_waypoints / step_count if step_count > 0 else 0
            
            # Calculate path smoothness (acceleration changes)
            velocity_commands = np.array(velocity_commands)
            if len(velocity_commands) > 1:
                vel_changes = np.diff(velocity_commands, axis=0)
                avg_vel_change = np.mean(np.linalg.norm(vel_changes, axis=1))
            else:
                avg_vel_change = 0
            
            # Calculate total distance traveled
            position_history = np.array(position_history)
            if len(position_history) > 1:
                distances = np.linalg.norm(np.diff(position_history, axis=0), axis=1)
                total_distance = np.sum(distances)
            else:
                total_distance = 0
            
            final_pos = position_history[-1] if len(position_history) > 0 else np.zeros(3)
            
            print(f"\n  --- Results ---")
            print(f"  Waypoints reached: {waypoints_reached}/{len(env.waypoints)} ({completion_rate:.1f}%)")
            print(f"  Steps completed: {step_count}/{scenario['duration']}")
            print(f"  Average distance to waypoint: {avg_dist_to_wp:.2f}m")
            print(f"  Total distance traveled: {total_distance:.2f}m")
            print(f"  Average velocity change: {avg_vel_change:.3f} m/s/step")
            print(f"  Final position: [{final_pos[0]:+.2f}, {final_pos[1]:+.2f}, {final_pos[2]:+.2f}]")
            print(f"  Crashed: {'YES' if terminated else 'NO'}")
            
            # Evaluate performance
            if completion_rate >= 80 and not terminated:
                status = "âœ… EXCELLENT"
            elif completion_rate >= 50 and not terminated:
                status = "âœ“ GOOD"
            elif completion_rate >= 30:
                status = "âš  FAIR"
            else:
                status = "âœ— POOR"
            print(f"  Performance: {status}")
            
            test_name = f"{scenario['name']}"
            if num_tests > 1:
                test_name += f" (Run {test_num + 1})"
            
            overall_results.append({
                'name': test_name,
                'completion': completion_rate,
                'crashed': terminated,
                'avg_dist': avg_dist_to_wp
            })
            
            time.sleep(2.0)  # Pause between scenarios
    
    env.close()
    
    # Overall summary
    print(f"\n{'='*80}")
    print("OVERALL COORDINATOR PERFORMANCE")
    print(f"{'='*80}")
    
    avg_completion = np.mean([r['completion'] for r in overall_results])
    crash_count = sum([1 for r in overall_results if r['crashed']])
    
    print(f"Average waypoint completion: {avg_completion:.1f}%")
    print(f"Scenarios crashed: {crash_count}/{len(overall_results)}")
    
    for result in overall_results:
        crash_marker = "ðŸ’¥" if result['crashed'] else "âœ“"
        print(f"  {crash_marker} {result['name']:20s}: {result['completion']:5.1f}% complete, "
              f"avg dist {result['avg_dist']:.2f}m")
    
    if avg_completion >= 70 and crash_count == 0:
        print("\nâœ… EXCELLENT: Coordinator navigates waypoints successfully!")
    elif avg_completion >= 50 and crash_count <= 1:
        print("\nâœ“ GOOD: Coordinator shows solid navigation performance")
    elif avg_completion >= 30:
        print("\nâš  FAIR: Coordinator needs improvement")
    else:
        print("\nâœ— POOR: Coordinator struggles with waypoint navigation")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    test_hierarchical_system()
